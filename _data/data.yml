#
# Be aware that even a small syntax error here can lead to failures in output.
#

sidebar:
    about: True # set to False or comment line if you want to remove the "how to use?" in the sidebar
    education: True # set to False if you want education in main section instead of in sidebar

    # Profile information
    name: JungKyoo Shin
    tagline: Ph.D Student
    avatar: Shin.jpg  #place a 100x100 picture inside /assets/images/ folder and provide the name of the file below

    # Sidebar links
    email: neo293@cau.ac.kr

    languages:
      title: Languages
      info:
        - idiom: English
          level: Professional

        - idiom: Korean
          level: Native

    
education:
    title: Education
    info:
      - degree: Ph.D in AI
        university: Chung-Ang University
        time: 2022 ~
      - degree: MS in AI
        university: UST, ETRI
        time: 2019 - 2021
      - degree: BS in information&communications and bioinformatics.
        university: Dongguk university
        time: 2015 - 2019
        



publications:
    title: Publications    
      - title: Negative Set Filtering for Contrastive Learning on Video-Text Retrieval
        link: "#"
        authors: Jungkyoo Shin, Eunwoo Kim
        conference: INTERNATIONAL CONFERENCE ON ELECTRONICS, INFORMATION, AND COMMUNICATION 2023
    intro: Contrastive learning has become a key component in video-text representation learning. In this paper, we argue an important aspect of contrastive learning, that not every negative is meaningful. Some negative sets can mislead the network and be detrimental to the learning process. We propose an algorithm to filter these harmful negative sets. Through extensive experiments on filtering, we study the negative sets that cause adverse influence. Experimental results show the influence of particular negative sets, thus achieving state-of-the-art performance on the video-text retrieval task using the YouCook2 dataset.
    
    papers:
      - title: Learning to combine the modalities of language and video for temporal moment localization
        link: "https://www.sciencedirect.com/science/article/pii/S1077314222000145"
        authors: Jungkyoo Shin, Jinyoung Moon
        conference: Computer Vision and Image Understanding Volume 217, March 2022, 103375
    intro: Temporal moment localization aims to retrieve the best video segment matching a moment specified by a query. The existing methods generate the visual and semantic embeddings independently and fuse them without full consideration of the long-term temporal relationship between them. To address these shortcomings, we introduce a novel recurrent unit, cross-modal long short-term memory (CM-LSTM), by mimicking the human cognitive process of localizing temporal moments that focuses on the part of a video segment related to the part of a query, and accumulates the contextual information across the entire video recurrently. In addition, we devise a two-stream attention mechanism for both attended and unattended video features by the input query to prevent necessary visual information from being neglected. To obtain more precise boundaries, we propose a two-stream attentive cross-modal interaction network (TACI) that generates two 2D proposal maps obtained globally from the integrated contextual features, which are generated by using CM-LSTM, and locally from boundary score sequences and then combines them into a final 2D map in an end-to-end manner. On the TML benchmark dataset, ActivityNet-Captions, the TACI outperforms state-of-the-art TML methods with R@1 of 45.50% and 27.23% for IoU@0.5 and IoU@0.7, respectively. In addition, we show that the revised state-of-the-arts methods by replacing original LSTM with our CM-LSTM achieves performance gains.

      - title: Multi-Perspective Attention Network for Fast Temporal Moment Localization
        link: "https://ieeexplore.ieee.org/abstract/document/9520423"
        authors: Jungkyoo Shin, Jinyoung Moon
        conference: IEEE ACCESS, 2022
    intro: Temporal moment localization (TML) aims to retrieve the temporal interval for a moment semantically relevant to a sentence query. This is challenging because it requires understanding a video, a sentence, and the relationship between them. Existing TML methods have shown impressive performances by modeling interactions between videos and sentences using fine-grained techniques. However, these fine-grained techniques require a high computational overhead, making them impractical. This work proposes an effective and efficient multi-perspective attention network for temporal moment localization. Inspired by the way humans understand an image from multiple perspectives and different contexts, we devise a novel multi-perspective attention mechanism consisting of perspective attention and multi-perspective modal interactions. Specifically, a perspective attention layer based on multi-head attention takes two memory sequences, one as the base and the other as the reference memory, as inputs. Perspective attention assesses the two different memories, models the relationship, and encourages the base memory to focus on features related to the reference memory, providing an understanding of the base memory from the perspective of the reference memory. Furthermore, multi-perspective modal interactions model the complex relationship between a video and sentence query, and obtain the modal-interacted memory, consisting of a visual feature that selectively learned query-related information. Similar to the heavyweight fine-grained TML methods, the proposed network obtains the accurate complex relationship while being lightweight like coarse-grained TML methods. We also adopt a fast action recognition network to efficiently extract visual features, which reduce the computational overhead. Through experiments on three TML benchmark datasets, we demonstrate the effectiveness and efficiency of the proposed network.


      - title: Fast Temporal Information Retrieval In Videos With Visual Memory
        link: "https://ieeexplore.ieee.org/abstract/document/9415226"
        authors: Jungkyoo Shin, Jinyoung Moon
        conference: ICAIIC 2021
    intro: Due to recent increases in video usage, there have been many studies about processing and managing information within huge volumes of videos. Existing methods for video retrieval aim to retrieve only similar frames related to a query image and compare all frames to the query image, which is costly in run-time and memory usage. To resolve these limitations, we propose a fast retrieval method for precise temporal information with visual memory. Our model compresses an input video into a compressed visual memory and applies an attention-based layer to obtain the probability of a given query image's existence. To the best of our knowledge, we are the first to attempt video retrieval for temporal information using visual memory. To show the efficiency and effectiveness of our model, we conducted experiments for temporal information retrieval on 60-second videos from TV shows and dramas. Our model could effectively compress a video to visual memory with space-savings of 93.6% and 99.1% compared to frame features and original video, respectively. Using the compressed visual memory, our method retrieved temporal information at 250K fps, which is 28x and 4,164x faster than retrieval methods using frame features and frames, respectively.



footer: >
    Designed with <i class="fas fa-heart"></i> by <a href="http://themes.3rdwavemedia.com" target="_blank" rel="nofollow">Xiaoying Riley</a>
